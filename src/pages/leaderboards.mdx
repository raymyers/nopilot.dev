---
title: Leaderboards
---
import Leaderboard from '@site/src/components/Leaderboard';

# Leaderboards

There are many LLM benchmarks, but for the purposes of evaluating Autonomous DevTools we are most interested in testing and agent's ability to address a realistic task on an existing codebase.

## SWE-bench verified
**The gold standard**. 
Released in September 2023 by Princeton NLP, SWE-bench is the most widely accepted measure of an agent's ability to solve tasks in a realistic codebase. 
It was constructed from GitHub Pull Requests from real Open Source respositories, with unit tests verifying the change. 
To pass, the agent must effectively recreate that Pull Request.

The full set is costly to run, we currently prefer the subset `SWE-bench verified`, which has been confirmed possible to solve by humans. Paul Gauthier [explains](https://github.com/princeton-nlp/SWE-bench/issues/72) the problem that led to that subset.

[SWE-bench](https://www.swebench.com) maintains the official leaderboard where results are reported.
Nopilot focuses on listing all reported scored by Open Source and Source Available agents.

<Leaderboard />

These are *unassisted* scores. "Assisted" means the agent is told which files need to be modified by the "oracle". There is usually a large difference between these scores, highlighting that navigating the codebase is a key part of the problem.

## Aider Leaderboards

The coding agent Aider maintains a [leaderboard](https://aider.chat/docs/leaderboards) of model performance within its key subtasks.

*Last checked: 2024-06-25*

### Code Editing

- claude-3.5-sonnet
- DeepSeek Coder V2 (Open Weight)
- gpt-4o
- claude-3-opus
- gpt-4
- deepseek-chat v2 (Open Weight)

### Code refactoring

- claude-3-opus
- gpt-4o
- gpt-4-turbo
- gemini-1.5-pro

Source for [Refactor Benchmark](https://github.com/paul-gauthier/refactor-benchmark).

## LiveCodeBench

[LiveCodeBench](https://livecodebench.github.io/leaderboard.html): "Holistic and Contamination Free Evaluation of Large Language Models for Code"

Tests the strength of models across different coding sub-tasks.

* Code Generation
* Self-Repair
* Test Output Prediction
* Code Execution

The below listing of standout models across subtasks is subjective.

*Last checked: 2024-06-25*
* Proprietary Leaders: , GPT-4o, GPT-4-Turbo, Claude-3-Opus, Claude-3.5-sonnet
* Open Weight Leaders:
  * [DeepSeekCoder-V2](https://huggingface.co/deepseek-ai/DeepSeek-V2)
  * [Codestral-Latest](https://mistral.ai/news/codestral/)
  * [LLama3-70b-Ins](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)
  * [Qwen2-Ins-72B](https://huggingface.co/Qwen/Qwen2-72B-Instruct)

## Other notable benchmarks

* [HumanEval](https://paperswithcode.com/sota/code-generation-on-humaneval) by OpenAI
